# Awesome Generate AI for Photography
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

With the rapid development of generative AI, photography, one of the most influential and widespread art forms, has experienced significant transformations. As a medium that bridges technology and humanity, photography is uniquely positioned to both shape and be shaped by recent generative models. We believe that photography and generative AI is an emerging research topic. In this research collection, we collected research works that integrate photographic principles, concepts, techniques, and domain knowledge into generative models, including those for image and video synthesis. These concepts are hard to be represented by simple text prompt. We hope this collection can inspire more research in this direction. 

We welcome all kinds of contributions. If you discover relevant new work, please feel free to contact the major maintainer, [I-Sheng Fang](https://ishengfang.github.io/).

## Contents
- [Papers by Years](#papers-by-year)
    - [2025](#2025)
        - [CVPR 2025](#cvpr-2025)
    - [2024](#2024)
        - [SIGGRAPH Asia 2024](#siggraph-asia-2024)
        - [ECCV 2024](#eccv-2024)
        - [CVPR 2024](#cvpr-2024)
- [Topics](#topics)
    - [Large Language Models](#large-language-models)
    - [Camera Settings](#camera-settings)
    - [Camera Trajectory](#camera-trajectory)

## Papers by Years

### 2025
- [arxiv 2025.03] **Trajectory Crafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models.** Yu *et al.*. [[paper]](https://arxiv.org/abs/2503.05638)[[Project Page]](https://trajectorycrafter.github.io/)
- [arxiv 2025.03] **Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models.** Fortes *et al.* [[Paper]](https://arxiv.org/abs/2503.08434)[[Project Page]](https://atfortes.github.io/projects/bokeh-diffusion/)[[Code]](https://github.com/atfortes/BokehDiffusion) 
#### CVPR 2025
- [CVPR 2025] **Towards Smart Point-and-Shoot Photography.** Li *et al.* [[paper]](https://arxiv.org/abs/2505.03638)
- [CVPR 2025] **The Photographer's Eye: Teaching Multimodal Large Language Models to See, Think and Critique Like Photographers.** Qi *et al.*
- [CVPR 2025] **Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding.** Zhao *et al.*
- [CVPR 2025] **Motion Prompting: Controlling Video Generation with Motion Trajectories.** Geng *et al.* [[Paper]](https://arxiv.org/abs/2412.02700)[[Project Page]](https://motion-prompting.github.io/)
- [CVPR 2025] **Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis.** Yuan *et al.* [[Paper]](https://arxiv.org/abs/2412.02168)[[Project Page]](https://generative-photography.github.io/project/)[[Dataset]](https://huggingface.co/datasets/pandaphd/camera_settings)[[Weights]](https://huggingface.co/pandaphd/generative_photography)[[Demo]](https://huggingface.co/spaces/pandaphd/generative_photography)
- [CVPR 2025] **PreciseCam: Precise Camera Control for Text-to-Image Generation.** Bernal-Berdun *et al.* [[Paper]](https://arxiv.org/abs/2501.12910)[[Project Page]](https://graphics.unizar.es/projects/PreciseCam2024/)
### 2024 
#### SIGGRAPH Asia 2024
- [SIGGRAPH Asia 2024] **Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models.** Fang *et al.*  [[Paper]](https://dl.acm.org/doi/10.1145/3680528.3687635)[[Project Page]](https://camera-settings-as-tokens.github.io/)[[Demo]](https://huggingface.co/spaces/Camera-Settings-as-Tokens/Camera-Settings-as-Tokens)[[Dataset]](https://github.com/aiiu-lab/CameraSettings20K)[[Model]](https://huggingface.co/ishengfang/Camera-Settings-as-Tokens-SD2)
- [SIGGRAPH Asia 2024] **Customizing Text-to-Image Diffusion with Camera Viewpoint Control.** Kumari *et al.* [[Paper]](http://arxiv.org/abs/2404.12333) [[Project Page]](https://customdiffusion360.github.io/)[[Demo]](https://huggingface.co/spaces/customdiffusion360/customdiffusion360)
#### ECCV 2024
- [ECCV 2024] **Viewpoint Textual Inversion: Discovering Scene Representations and 3D View Control in 2D Diffusion Models.** Burgess *et al.* [[Paper]](https://arxiv.org/abs/2309.07986)[[Project Page]](https://jmhb0.github.io/view_neti/)

#### CVPR 2024
- [CVPR 2024] **Learning Continuous 3D Words for Text-to-Image Generation.** Cheng *et al.* [[Paper]](https://ttchengab.github.io/continuous_3d_words/c3d_words.pdf)[[Project Page]](https://ttchengab.github.io/continuous_3d_words/)[[Code]](https://github.com/ttchengab/continuous_3d_words_code/)
- [CVPR 2024] **ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models.** Höllein *et al.* [[Paper]](https://arxiv.org/abs/2403.01807)[[Project Page]](https://lukashoel.github.io/ViewDiff/)[[Code]](https://github.com/facebookresearch/ViewDiff)

### 2023
- [arxiv 2023.10] **CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models.** Yuan *et al.* [[Paper]](https://arxiv.org/abs/2310.19784)[[Project Page]](https://jiangyzy.github.io/CustomNet/)

## Topics
### Large Language Models
- [CVPR 2025] **The Photographer's Eye: Teaching Multimodal Large Language Models to See, Think and Critique Like Photographers.** Qi *et al.*
- [CVPR 2025] **Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding.** Zhao *et al.*

### Camera Settings
- [arxiv 2025.03] **Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models.** Fortes *et al.* [[Paper]](https://arxiv.org/abs/2503.08434) [[Project Page]](https://atfortes.github.io/projects/bokeh-diffusion/)[[Code]](https://github.com/atfortes/BokehDiffusion) 
- [CVPR 2025] **PreciseCam: Precise Camera Control for Text-to-Image Generation.** Bernal-Berdun *et al.* [[Paper]](https://arxiv.org/abs/2501.12910)[[Project Page]](https://graphics.unizar.es/projects/PreciseCam2024/)
- [CVPR 2025]  **Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis.** Yuan *et al.* [[Paper]](https://arxiv.org/abs/2412.02168)[[Project Page]](https://generative-photography.github.io/project/)[[Dataset]](https://huggingface.co/datasets/pandaphd/camera_settings) [[Weights]](https://huggingface.co/pandaphd/generative_photography)[[Demo]](https://huggingface.co/spaces/pandaphd/generative_photography)
- [SIGGRAPH Asia 2024] **Camera Settings as Tokens: Modeling Photography on Latent Diffusion Models.** Fang *et al.*  [[Paper]](https://dl.acm.org/doi/10.1145/3680528.3687635)[[Project Page]](https://camera-settings-as-tokens.github.io/)[[Demo]](https://huggingface.co/spaces/Camera-Settings-as-Tokens/Camera-Settings-as-Tokens)[[Dataset]](https://github.com/aiiu-lab/CameraSettings20K)[[Model]](https://huggingface.co/ishengfang/Camera-Settings-as-Tokens-SD2)

### Camera Trajectory / Pose

- [arxiv 2025.03] **Trajectory Crafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models.** Yu *et al.*. [[paper]](https://arxiv.org/abs/2503.05638)[[Project Page]](https://trajectorycrafter.github.io/)
- [CVPR 2025] **Towards Smart Point-and-Shoot Photography.** Li *et al.* [[paper]](https://arxiv.org/abs/2505.03638)
- [CVPR 2025] **Motion Prompting: Controlling Video Generation with Motion Trajectories.** Geng *et al.* [[Paper]](https://arxiv.org/abs/2412.02700)[[Project Page]](https://motion-prompting.github.io/)
- [SIGGRAPH Asia 2024] **Customizing Text-to-Image Diffusion with Camera Viewpoint Control.** Kumari *et al.* [[Paper]](http://arxiv.org/abs/2404.12333) [[Project Page]](https://customdiffusion360.github.io/)[[Demo]](https://huggingface.co/spaces/customdiffusion360/customdiffusion360)
- [CVPR 2024] **Learning Continuous 3D Words for Text-to-Image Generation.** Cheng *et al.* [[Paper]](https://ttchengab.github.io/continuous_3d_words/c3d_words.pdf)[[Project Page]](https://ttchengab.github.io/continuous_3d_words/)[[Code]](https://github.com/ttchengab/continuous_3d_words_code/)
- [CVPR 2024] **ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models.** Höllein *et al.* [[Paper]](https://arxiv.org/abs/2403.01807)[[Project Page]](https://lukashoel.github.io/ViewDiff/)[[Code]](https://github.com/facebookresearch/ViewDiff)
- [arxiv 2023.10] **CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models.** Yuan *et al.* [[Paper]](https://arxiv.org/abs/2310.19784)[[Project Page]](https://jiangyzy.github.io/CustomNet/)
-
